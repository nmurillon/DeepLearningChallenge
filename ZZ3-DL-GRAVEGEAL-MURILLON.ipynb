{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab-github-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmurillon/DeepLearningChallenge/blob/main/ZZ3-DL-GRAVEGEAL-MURILLON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYaVmD0KO0Y_"
      },
      "source": [
        "##Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OciXqbLO-9o"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from PIL import Image\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Ajouter les imports nécessaires\n",
        "import os #Changer le répertoire courant pour les chemins des fichiers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dEPXagXUNvF"
      },
      "source": [
        "##Variables globales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3emCpSfFPSb4"
      },
      "source": [
        "num_epochs = 100\n",
        "batch_size = 250\n",
        "\n",
        "img_shape = (28,28,1)\n",
        "img_size = img_shape[:2]\n",
        "\n",
        "num_classes = 5\n",
        "stop_freeze = 4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-kjtXlnPHeV"
      },
      "source": [
        "##Données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRanVV2fWk7w",
        "outputId": "7bb45e9f-cd68-471c-b1e3-d4ca26ba5a8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#On monte le drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxk7md__YTyd"
      },
      "source": [
        "#on change le répertoire courant pour faciliter l'accès aux données\n",
        "os.chdir('/content/drive/MyDrive/DeepLearning/') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwiFJxf2PGb2"
      },
      "source": [
        "classes = [\"basket\",\"eye\",\"binoculars\",\"rabbit\",\"hand\"]\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "valid = pd.read_csv('valid.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNLjv2bcPQKW"
      },
      "source": [
        "plt.figure(figsize=(18,9))\n",
        "for i in range(0,5):\n",
        "    ax= plt.subplot(3,2 ,i+1)\n",
        "    mydata = pd.read_csv(\"train.csv\",skiprows = [1], nrows=1)\n",
        "    im = Image.open('images/'+classes[i]+'/'+ os.listdir(\"images/\"+classes[i])[0])\n",
        "    fig=ax.imshow(im)\n",
        "    plt.title(classes[i])\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)\n",
        "plt.show()    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FobETo7bd03a"
      },
      "source": [
        "# TODO\n",
        "def Data():\n",
        "    data_dir = os.getcwd()\n",
        "    data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "    image_number = len(list(data_dir.glob('*/*.jpg')))\n",
        "    print(\"Nombre d'images :\", image_number)\n",
        "    classes = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "    print(\"Classes : \",classes)\n",
        "\n",
        "    # Augmentation de données : 75% entraînement, 25% test\n",
        "    image_gen = ImageDataGenerator(\n",
        "        rescale = 1./255, \n",
        "        # rotation_range=10, # rotation\n",
        "        # zoom_range=0.2, # zoom\n",
        "        # horizontal_flip=True, # horizontal flip\n",
        "        # brightness_range=[0.2,1.2], # brightness\n",
        "        validation_split=0.25) \n",
        "        \n",
        "    train_data_gen = image_gen.flow_from_directory(data_dir, target_size=image_size, color_mode='rgb', shuffle = True, class_mode='categorical', subset='training')\n",
        "    # Augmentation de la base de test/validation\n",
        "    test_data_gen = image_gen.flow_from_directory(data_dir, target_size=image_size, color_mode='rgb', shuffle = True, class_mode='categorical', subset='validation')\n",
        "\n",
        "    return train_data_gen, test_data_gen, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1CnH2qNPocd"
      },
      "source": [
        "##Modèle 1 : Fine tuning\n",
        "\n",
        "Dans un premier temps, nous utiliserons le transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XL2GYphQFBH"
      },
      "source": [
        "def TransferModel(input_shape,stop_freeze, model):\n",
        "    '''\n",
        "    @param model is a model from tensorflow.keras.applications\n",
        "    '''\n",
        "    pretrained_model = MobileNetV2(input_shape=image_shape)\n",
        "    \n",
        "    # On ôte la dernière couche de classification\n",
        "    pretrained_model.layers.pop()   \n",
        "    \n",
        "    inputs = Input(input_shape)\n",
        "    x = pretrained_model(inputs)\n",
        "    # On fige tous les poids sauf ceux des stop_freeze dernières couches\n",
        "    for i in range(len(pretrained_model.layers) - stop_freeze):\n",
        "      pretrained_model.layers[i].trainable = False\n",
        "    \n",
        "    model = Sequential([pretrained_model,\n",
        "                        Dense(num_classes, activation='softmax')])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGHAD5lGZDSt"
      },
      "source": [
        "##Entraînement du réseau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7PSLLccZBuu"
      },
      "source": [
        "#x_train, x_test = Data()\n",
        "#pretrained_model = # TODO\n",
        "#pretrained_model_name = \"\"\n",
        "model = TransferModel(img_shape, stop_freeze, pretrained_model)\n",
        "\n",
        "# Callback pour la sauvegarde du meilleur modèle\n",
        "if not os.path.isdir(\"sauve\"):\n",
        "    os.mkdir(\"sauve\")\n",
        "    \n",
        "checkpoint = ModelCheckpoint(f\"sauve/{pretrained_model_name}-loss-{val_loss:.2f}-acc-{accuracy:.2f}.h5\",\n",
        "                                save_best_only=True,verbose=1)\n",
        "\n",
        "\n",
        "# TODO !!!\n",
        "train_steps_epoch = np.ceil(x_train.samples / batch_size)\n",
        "val_steps_epoch = np.ceil(x_test.samples / batch_size)\n",
        "\n",
        "# Entraînement\n",
        "model.fit(x_train, steps_per_epoch=train_steps_epoch,\n",
        "                        validation_data=x_test, validation_steps=val_steps_epoch,\n",
        "                        epochs=num_epochs, verbose=1, callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}